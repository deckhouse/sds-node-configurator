{{- define "sds_utils_installer_resources" }}
cpu: 10m
memory: 25Mi
{{- end }}

{{- define "sds_node_configurator_agent_resources" }}
cpu: 50m
memory: 50Mi
{{- end }}

{{- define "storage_network_discoverer_resources" }}
cpu: 50m
memory: 25Mi
{{- end }}

{{- if not .Values.sdsNodeConfigurator.disableDs }}
---
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: sds-node-configurator
  namespace: d8-{{ .Chart.Name }}
  {{- include "helm_lib_module_labels" (list . (dict "app" "sds-node-configurator")) | nindent 2 }}
spec:
  selector:
    matchLabels:
      app: sds-node-configurator
  template:
    metadata:
      name: sds-node-configurator
      namespace: d8-{{ .Chart.Name }}
      {{- include "helm_lib_module_labels" (list . (dict "app" "sds-node-configurator")) | nindent 6 }}
    spec:
      {{- include "helm_lib_priority_class" (tuple . "cluster-medium") | nindent 6 }}
      {{- include "helm_lib_tolerations" (tuple . "any-node" "storage-problems") | nindent 6 }}
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
              - matchExpressions:
                  - key: storage.deckhouse.io/sds-replicated-volume-node
                    operator: In
                    values:
                      - ""
              - matchExpressions:
                  - key: storage.deckhouse.io/sds-local-volume-node
                    operator: In
                    values:
                      - ""
              - matchExpressions:
                  - key: storage.deckhouse.io/sds-drbd-node
                    operator: In
                    values:
                      - ""
      imagePullSecrets:
        - name: {{ .Chart.Name }}-module-registry
      serviceAccountName: sds-node-configurator
      hostPID: true
      hostNetwork: true
      # We need root privileges to perform LVM operations on the node.
      securityContext:
        runAsUser: 0
        runAsNonRoot: false
        runAsGroup: 0
        seLinuxOptions:
          level: s0
          type: spc_t
      initContainers:
      - name: sds-utils-installer
        image: {{ include "helm_lib_module_image" (list . "sdsUtilsInstaller") }}
        imagePullPolicy: IfNotPresent
        volumeMounts:
        - mountPath: /opt/deckhouse/sds
          name: opt-deckhouse-sds
        resources:
          requests:
            {{- include "helm_lib_module_ephemeral_storage_only_logs" . | nindent 14 }}
{{- if not ( .Values.global.enabledModules | has "vertical-pod-autoscaler-crd") }}
            {{- include "sds_utils_installer_resources" . | nindent 14 }}
{{- end }}
{{- if .Values.sdsNodeConfigurator.enableThinProvisioning }}
      - name: thin-volumes-enabler
        image: {{ include "helm_lib_module_image" (list . "agent") }}
        imagePullPolicy: IfNotPresent
        command:
          - /opt/deckhouse/sds/bin/nsenter.static
          - -t
          - "1"
          - -m
          - -u
          - -i
          - -n
          - -p
          - --
          - modprobe
          - -a
          - dm_thin_pool
{{- if  (.Values.global.enabledModules | has "snapshot-controller") }}
          - dm_snapshot
{{- end }}
        # Privileged mode is required to use nsenter and access the host's mount namespace.
        # This is necessary to run modprobe and load the dm_thin_pool kernel module on the host.
        securityContext:
          privileged: true
        volumeMounts:
          - mountPath: /dev/
            name: host-device-dir
          - mountPath: /sys/
            name: host-sys-dir
          - mountPath: /run/udev/
            name: host-run-udev-dir
        resources:
          requests:
            {{- include "helm_lib_module_ephemeral_storage_only_logs" . | nindent 14 }}
{{- if not ( .Values.global.enabledModules | has "vertical-pod-autoscaler-crd") }}
            {{- include "sds_utils_installer_resources" . | nindent 14 }}
{{- end }}
{{- end }}
      containers:
      - name: sds-node-configurator-agent
        image: {{ include "helm_lib_module_image" (list . "agent") }}
        imagePullPolicy: IfNotPresent
        readinessProbe:
          httpGet:
            path: /readyz
            port: 4228
            scheme: HTTP
          initialDelaySeconds: 5
          failureThreshold: 2
          periodSeconds: 1
        livenessProbe:
          httpGet:
            path: /healthz
            port: 4228
            scheme: HTTP
          periodSeconds: 1
          failureThreshold: 3
        ports:
          - name: metrics
            containerPort: 4202
            protocol: TCP
        env:
          - name: NODE_NAME
            valueFrom:
              fieldRef:
                fieldPath: spec.nodeName
          - name: LOG_LEVEL
{{- if eq .Values.sdsNodeConfigurator.logLevel "ERROR" }}
            value: "0"
{{- else if eq .Values.sdsNodeConfigurator.logLevel "WARN" }}
            value: "1"
{{- else if eq .Values.sdsNodeConfigurator.logLevel "INFO" }}
            value: "2"
{{- else if eq .Values.sdsNodeConfigurator.logLevel "DEBUG" }}
            value: "3"
{{- else if eq .Values.sdsNodeConfigurator.logLevel "TRACE" }}
            value: "4"
{{- end }}
        # Privileged mode is required to use nsenter and execute host-level commands like lvm and lsblk.
        securityContext:
          privileged: true
        volumeMounts:
        - mountPath: /dev/
          name: host-device-dir
        - mountPath: /sys/
          name: host-sys-dir
        - mountPath: /run/udev/
          name: host-run-udev-dir
        resources:
          requests:
            {{- include "helm_lib_module_ephemeral_storage_only_logs" . | nindent 14 }}
{{- if not ( .Values.global.enabledModules | has "vertical-pod-autoscaler-crd") }}
            {{- include "sds_node_configurator_agent_resources" . | nindent 14 }}
{{- end }}
      {{- if .Values.sdsNodeConfigurator.storageNetworkCIDRs }}
      - name: discoverer
        image: {{ include "helm_lib_module_image" (list . "storageNetworkDiscoverer") }}
        imagePullPolicy: IfNotPresent
        args:
          {{- range $cidr := .Values.sdsNodeConfigurator.storageNetworkCIDRs }}
          - --storage-network-cidr
          - {{ $cidr | quote }}
          {{- end }}
        env:
          - name: LOG_LEVEL
{{- if eq .Values.sdsNodeConfigurator.logLevel "ERROR" }}
            value: "0"
{{- else if eq .Values.sdsNodeConfigurator.logLevel "WARN" }}
            value: "1"
{{- else if eq .Values.sdsNodeConfigurator.logLevel "INFO" }}
            value: "2"
{{- else if eq .Values.sdsNodeConfigurator.logLevel "DEBUG" }}
            value: "3"
{{- else if eq .Values.sdsNodeConfigurator.logLevel "TRACE" }}
            value: "4"
{{- end }}
          # try discover storage IPs on each node every 30s
          - name: DISCOVERY_INTERVAL_SEC
            value: "30"
          # If founded, storage IP will be set to node's status and cached for CACHE_TTL_SEC
          # Discoverer DO NOT attempt to update node before this cache will expired
          # So, do not set this value too high. (2 * DISCOVERY_INTERVAL_SEC) should be enough.
          - name: CACHE_TTL_SEC
            value: "60"
          # NODE_NAME REQUIRED by discoverer
          - name: NODE_NAME
            valueFrom:
              fieldRef:
                fieldPath: spec.nodeName
          - name: POD_IP
            valueFrom:
              fieldRef:
                fieldPath: status.podIP
        resources:
          requests:
            {{- include "helm_lib_module_ephemeral_storage_only_logs" . | nindent 12 }}
          {{- if not ( .Values.global.enabledModules | has "vertical-pod-autoscaler-crd") }}
            {{- include "storage_network_discoverer_resources" . | nindent 12 }}
          {{- end }}
      {{- end }}

      volumes:
      - hostPath:
          path: /opt/deckhouse/sds
          type: DirectoryOrCreate
        name: opt-deckhouse-sds
      - hostPath:
          path: /dev/
          type: ""
        name: host-device-dir
      - hostPath:
          path: /sys/
          type: Directory
        name: host-sys-dir
      - hostPath:
          path: /run/udev/
          type: Directory
        name: host-run-udev-dir
{{- end }}
